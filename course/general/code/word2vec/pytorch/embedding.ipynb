{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5492, -1.1496,  1.2080],\n",
       "         [-0.6645,  0.8390, -0.8144],\n",
       "         [ 0.5169,  0.4132, -0.0543],\n",
       "         [ 1.6722, -1.2496, -0.3160]],\n",
       "\n",
       "        [[ 0.5169,  0.4132, -0.0543],\n",
       "         [ 0.8651, -0.0635,  0.6112],\n",
       "         [-0.6645,  0.8390, -0.8144],\n",
       "         [ 0.1893,  1.1009,  1.4389]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.8590,  1.3562,  0.0523],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.6379,  0.9526,  0.0189]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ngram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ac71df0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]])\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
      "[520.4697849750519, 517.9997153282166, 515.5484156608582, 513.1148540973663, 510.69636726379395, 508.29099345207214, 505.8987572193146, 503.520117521286, 501.15412640571594, 498.79959201812744]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 245.5927]), tensor([ 240.6575]), tensor([ 235.8352]), tensor([ 231.1232]), tensor([ 226.5193]), tensor([ 222.0219]), tensor([ 217.6293]), tensor([ 213.3403]), tensor([ 209.1534]), tensor([ 205.0673]), tensor([ 201.0808]), tensor([ 197.1924]), tensor([ 193.4006]), tensor([ 189.7036]), tensor([ 186.0997]), tensor([ 182.5868]), tensor([ 179.1627]), tensor([ 175.8253]), tensor([ 172.5721]), tensor([ 169.4005]), tensor([ 166.3081]), tensor([ 163.2923]), tensor([ 160.3506]), tensor([ 157.4805]), tensor([ 154.6795]), tensor([ 151.9454]), tensor([ 149.2757]), tensor([ 146.6684]), tensor([ 144.1213]), tensor([ 141.6325]), tensor([ 139.2001]), tensor([ 136.8223]), tensor([ 134.4973]), tensor([ 132.2236]), tensor([ 129.9996]), tensor([ 127.8238]), tensor([ 125.6947]), tensor([ 123.6111]), tensor([ 121.5715]), tensor([ 119.5749]), tensor([ 117.6199]), tensor([ 115.7055]), tensor([ 113.8305]), tensor([ 111.9941]), tensor([ 110.1950]), tensor([ 108.4325]), tensor([ 106.7056]), tensor([ 105.0134]), tensor([ 103.3551]), tensor([ 101.7299]), tensor([ 100.1371]), tensor([ 98.5759]), tensor([ 97.0456]), tensor([ 95.5455]), tensor([ 94.0750]), tensor([ 92.6334]), tensor([ 91.2201]), tensor([ 89.8345]), tensor([ 88.4760]), tensor([ 87.1441]), tensor([ 85.8382]), tensor([ 84.5577]), tensor([ 83.3022]), tensor([ 82.0710]), tensor([ 80.8638]), tensor([ 79.6800]), tensor([ 78.5190]), tensor([ 77.3805]), tensor([ 76.2639]), tensor([ 75.1688]), tensor([ 74.0948]), tensor([ 73.0413]), tensor([ 72.0079]), tensor([ 70.9942]), tensor([ 69.9998]), tensor([ 69.0242]), tensor([ 68.0670]), tensor([ 67.1279]), tensor([ 66.2064]), tensor([ 65.3021]), tensor([ 64.4147]), tensor([ 63.5438]), tensor([ 62.6889]), tensor([ 61.8499]), tensor([ 61.0262]), tensor([ 60.2176]), tensor([ 59.4237]), tensor([ 58.6442]), tensor([ 57.8788]), tensor([ 57.1271]), tensor([ 56.3890]), tensor([ 55.6640]), tensor([ 54.9519]), tensor([ 54.2524]), tensor([ 53.5653]), tensor([ 52.8903]), tensor([ 52.2271]), tensor([ 51.5755]), tensor([ 50.9352]), tensor([ 50.3060])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "torch.manual_seed(1)\n",
    " \n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    " \n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    " \n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    " \n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW,self).__init__() \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    " \n",
    " \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        add_embeds = torch.sum(embeds, dim=0).view(1,-1)\n",
    "        out = self.linear1(add_embeds)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs \n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    " \n",
    "\n",
    "# make_context_vector(data[0][0], word_to_ix)\n",
    " \n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embedding_dim=20)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    " \n",
    "for epoch in range(100):\n",
    "    total_loss = torch.FloatTensor([0])\n",
    "    for context, target in data:\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        target_idx = word_to_ix[target]\n",
    "        context_var = Variable(torch.LongTensor(context_idxs))\n",
    "        target_var = Variable(torch.LongTensor([target_idx]))\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_var)\n",
    " \n",
    "        loss = loss_function(log_probs, target_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e7ac080>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d49a3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), [float(x) for x in losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
