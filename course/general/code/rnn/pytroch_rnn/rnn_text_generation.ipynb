{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RNN for text generation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a language model based on Shakespear's writings, and use it to generate new text similar to that of Shakespear."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.cuda as cuda\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper class to read in the texts, convert the words to integer indexes and provide lookup tables to convert any word to it's index and vice versa**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    \n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        \n",
        "        # This is very english language specific\n",
        "        # We will ingest only these characters:\n",
        "        self.whitelist = [chr(i) for i in range(32, 127)]\n",
        "        \n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                line = ''.join([c for c in line if c in self.whitelist])\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                line = ''.join([c for c in line if c in self.whitelist])\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n        return ids"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/shakespear"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.txt  valid.txt\r\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus('./data/shakespear')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus.dictionary.idx2word[10])\n",
        "print(corpus.dictionary.word2idx['That'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That\n",
            "10\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus.train.size())\n",
        "print(corpus.valid.size())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1039900])\n",
            "torch.Size([63420])\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "id = corpus.train[112]\n",
        "corpus.dictionary.idx2word[id]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": [
              "'else'"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(corpus.dictionary)\n",
        "print(vocab_size)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74010\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The RNN model (using GRU cells)**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop1(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop2(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    if cuda.is_available():\n",
        "        data = data.cuda()\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data = \"Once upon a time there was a good king and a queen\"\n",
        "dummy_data_idx = [corpus.dictionary.word2idx[w] for w in dummy_data.split()]\n",
        "dummy_tensor = torch.LongTensor(dummy_data_idx) \n",
        "op = batchify(dummy_tensor, 2)\n",
        "for row in op:\n",
        "    print(\"%10s %10s\" %  (corpus.dictionary.idx2word[row[0]], corpus.dictionary.idx2word[row[1]]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Once          a\n",
            "      upon       good\n",
            "         a       king\n",
            "      time        and\n",
            "     there          a\n",
            "       was      queen\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bs_train = 20       # batch size for training set\n",
        "bs_valid = 10       # batch size for validation set\n",
        "bptt_size = 35      # number of times to unroll the graph for back propagation through time\n",
        "clip = 0.25         # gradient clipping to check exploding gradient\n",
        "\n",
        "embed_size = 200    # size of the embedding vector\n",
        "hidden_size = 200   # size of the hidden state in the RNN \n",
        "num_layers = 2      # number of RNN layres to use\n",
        "dropout_pct = 0.5   # %age of neurons to drop out for regularization"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = batchify(corpus.train, bs_train)\n",
        "val_data = batchify(corpus.valid, bs_valid)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": [
              "torch.Size([51995, 20])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
        "\n",
        "if cuda.is_available():\n",
        "    model.cuda()"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(source, i, evaluation=False):\n",
        "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
        "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
        "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
        "    if cuda.is_available():\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "    return data, target"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data, target = get_batch(train_data, 1)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": [
              "torch.Size([35, 20])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": [
              "torch.Size([700])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_source, lr):\n",
        "    # Turn on training mode which enables dropout.\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    hidden = model.init_hidden(bs_train)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt_size)):\n",
        "        \n",
        "        data, targets = get_batch(data_source, i)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = Variable(hidden.data)\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "        \n",
        "        # model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.data\n",
        "        \n",
        "    return total_loss[0] / len(data_source)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    hidden = model.init_hidden(bs_valid)\n",
        "    \n",
        "    for i in range(0, data_source.size(0) - 1, bptt_size):\n",
        "        data, targets = get_batch(data_source, i, evaluation=True)\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "            \n",
        "        output, hidden = model(data, hidden)\n",
        "        output_flat = output.view(-1, vocab_size)\n",
        "        \n",
        "        total_loss += len(data) * criterion(output_flat, targets).data\n",
        "        hidden = Variable(hidden.data)\n",
        "        \n",
        "    return total_loss[0] / len(data_source)\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def run(epochs, lr):\n",
        "    global best_val_loss\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        train_loss = train(train_data, lr)\n",
        "        val_loss = evaluate(val_data)\n",
        "        print(\"Train Loss: \", train_loss, \"Valid Loss: \", val_loss)\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"./4.model.pth\")\n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run(5, 0.001)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  6.879061688623906 Valid Loss:  6.883903934090192\n",
            "Train Loss:  6.354372415616886 Valid Loss:  6.777434908152003\n",
            "Train Loss:  6.1925641888643135 Valid Loss:  6.713705504966887\n",
            "Train Loss:  6.090176940090394 Valid Loss:  6.698393399164302\n",
            "Train Loss:  6.017398908548899 Valid Loss:  6.7134160162409335\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run(5, 0.001)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss:  5.9627584383113765 Valid Loss:  6.722295928926206\n",
            "Train Loss:  5.916144581209732 Valid Loss:  6.725216069457584\n",
            "Train Loss:  5.878729925954419 Valid Loss:  6.75531858542258\n",
            "Train Loss:  5.848820199057601 Valid Loss:  6.768370831362346\n",
            "Train Loss:  5.825219372055005 Valid Loss:  6.7805644660596025\n"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 200\n",
        "temperature = 1"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
        "model.load_state_dict(torch.load(\"./4.model.pth\"))\n",
        "\n",
        "if cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "model.eval()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (encoder): Embedding(74010, 200)\n",
              "  (drop1): Dropout(p=0.5)\n",
              "  (drop2): Dropout(p=0.5)\n",
              "  (rnn): GRU(200, 200, num_layers=2, dropout=0.5)\n",
              "  (decoder): Linear(in_features=200, out_features=74010, bias=True)\n",
              ")"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "# Which sample is better? It depends on your personal taste. The high temperature \n",
        "# sample displays greater linguistic variety, but the low temperature sample is \n",
        "# more grammatically correct. Such is the world of temperature sampling - lowering \n",
        "# the temperature allows you to focus on higher probability output sequences and \n",
        "# smooth over deficiencies of the model.\n",
        "\n",
        "# If we set a high temperature, we can get more entropic (*noisier*) probabilities\n",
        "# Often we want to sample with low temperatures to produce sharp probabilities\n",
        "temperature = 0.8"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = model.init_hidden(1)\n",
        "idx = corpus.dictionary.word2idx['I']\n",
        "input = Variable(torch.LongTensor([[idx]]).long(), volatile=True)\n",
        "\n",
        "if cuda.is_available():\n",
        "    input.data = input.data.cuda()\n",
        "\n",
        "print(corpus.dictionary.idx2word[idx], '', end='')\n",
        "\n",
        "for i in range(num_words):\n",
        "    output, hidden = model(input, hidden)\n",
        "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.data.fill_(word_idx)\n",
        "    word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "    if word == '<eos>':\n",
        "        print('')\n",
        "    else:\n",
        "        print(word + ' ', end='')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a cog, \n",
            "A trumpet with his sham'd. \n",
            "SECOND MURDERER. I have say make you be of your now. \n",
            "PETER. He shall not be afeard against my life, for her, to the \n",
            "music of my name to a thence is the Queen of the \n",
            "tanner of my own posterity, \n",
            "WOLSEY. I know the matter, Is a Boult, an steward \n",
            "I'll hear the mercy when they are you? \n",
            "The valour is the Empress' innocent \n",
            "At heaven unto the flesh, have a Antony, \n",
            "Yet, like the man of high all men will am \n",
            "And nothing to faith, the happiness of his war. \n",
            "FIRST MURDERER. What goes the thousand king of his man's minds in \n",
            "reason. \n",
            "\n",
            "Come on him here. \n",
            "\n",
            "Enter PISANIO and drum and LORD \n",
            "\n",
            "CAPULET. \n",
            "My poor lord, I are a letter in \n",
            "To see them. Let the King be absent. \n",
            "I am that woman, can not into a weapon \n",
            "To us a thousand more. \n",
            "\n",
            "Set of the master? \n",
            "\n",
            "HAMLET. \n",
            "How bright my council lie up to the exceeds and cross \n",
            "bade "
          ]
        }
      ],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Things to explore**\n",
        "\n",
        "* Play with the hyperparameters\n",
        "* Play with the model architecture\n",
        "* Run this on a different dataset\n",
        "* Experiment with attention\n",
        "* Compare perplexity"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}